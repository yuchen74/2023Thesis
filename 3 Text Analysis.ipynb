{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "central-offering",
   "metadata": {},
   "source": [
    "# Text Analysis on gurlitt data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "material-stopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "stupid-explanation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'D:\\論文\\PHEME9\\Data\\CSV\\gurlitt.csv', encoding = 'utf-8', header = None)\n",
    "df.columns = ['src_tweet_id', 'src_user_id', 'src_tweet','src_date','reply_tweet_id','reply_user_id','reply_tweet','reply_date', 'label']\n",
    "df.reply_tweet = df.reply_tweet.replace(np.nan,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "brave-category",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_tweet_id</th>\n",
       "      <th>src_user_id</th>\n",
       "      <th>src_tweet</th>\n",
       "      <th>src_date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>535386605666979840</td>\n",
       "      <td>43092938</td>\n",
       "      <td>Will Bern's Museum of Fine Arts accept the con...</td>\n",
       "      <td>Thu Nov 20 10:58:08 +0000 2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>535391478969675776</td>\n",
       "      <td>38402632</td>\n",
       "      <td>The Gurlitt art collection no one - and everyo...</td>\n",
       "      <td>Thu Nov 20 11:17:30 +0000 2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>535415010361487360</td>\n",
       "      <td>719898644</td>\n",
       "      <td>German and Swiss handling of Gurlitt hoard and...</td>\n",
       "      <td>Thu Nov 20 12:51:00 +0000 2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>535415869862473729</td>\n",
       "      <td>1140049158</td>\n",
       "      <td>The Gurlitt art collection no one - and everyo...</td>\n",
       "      <td>Thu Nov 20 12:54:25 +0000 2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>535425236871544832</td>\n",
       "      <td>289989248</td>\n",
       "      <td>The Gurlitt art collection no one - and everyo...</td>\n",
       "      <td>Thu Nov 20 13:31:38 +0000 2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>536848134681690112</td>\n",
       "      <td>22138756</td>\n",
       "      <td>Summary of the #Gurlitt agreement between Kuns...</td>\n",
       "      <td>Mon Nov 24 11:45:44 +0000 2014</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>536848139836481536</td>\n",
       "      <td>16541374</td>\n",
       "      <td>Gurlitt collection accepted by Kunstmuseum Ber...</td>\n",
       "      <td>Mon Nov 24 11:45:45 +0000 2014</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>536848241556721665</td>\n",
       "      <td>553661747</td>\n",
       "      <td>Swiss museum accepts looted Nazi art http://t....</td>\n",
       "      <td>Mon Nov 24 11:46:09 +0000 2014</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>536848662135980032</td>\n",
       "      <td>59608915</td>\n",
       "      <td>Gurlitt Collection Accepted by Kunstmuseum Ber...</td>\n",
       "      <td>Mon Nov 24 11:47:49 +0000 2014</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>536848962775678976</td>\n",
       "      <td>442587695</td>\n",
       "      <td>@KunstmuseumBern declares that it will accept ...</td>\n",
       "      <td>Mon Nov 24 11:49:01 +0000 2014</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>138 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           src_tweet_id  src_user_id  \\\n",
       "0    535386605666979840     43092938   \n",
       "1    535391478969675776     38402632   \n",
       "2    535415010361487360    719898644   \n",
       "3    535415869862473729   1140049158   \n",
       "4    535425236871544832    289989248   \n",
       "..                  ...          ...   \n",
       "159  536848134681690112     22138756   \n",
       "160  536848139836481536     16541374   \n",
       "161  536848241556721665    553661747   \n",
       "162  536848662135980032     59608915   \n",
       "163  536848962775678976    442587695   \n",
       "\n",
       "                                             src_tweet  \\\n",
       "0    Will Bern's Museum of Fine Arts accept the con...   \n",
       "1    The Gurlitt art collection no one - and everyo...   \n",
       "2    German and Swiss handling of Gurlitt hoard and...   \n",
       "3    The Gurlitt art collection no one - and everyo...   \n",
       "4    The Gurlitt art collection no one - and everyo...   \n",
       "..                                                 ...   \n",
       "159  Summary of the #Gurlitt agreement between Kuns...   \n",
       "160  Gurlitt collection accepted by Kunstmuseum Ber...   \n",
       "161  Swiss museum accepts looted Nazi art http://t....   \n",
       "162  Gurlitt Collection Accepted by Kunstmuseum Ber...   \n",
       "163  @KunstmuseumBern declares that it will accept ...   \n",
       "\n",
       "                           src_date  label  \n",
       "0    Thu Nov 20 10:58:08 +0000 2014      0  \n",
       "1    Thu Nov 20 11:17:30 +0000 2014      0  \n",
       "2    Thu Nov 20 12:51:00 +0000 2014      0  \n",
       "3    Thu Nov 20 12:54:25 +0000 2014      0  \n",
       "4    Thu Nov 20 13:31:38 +0000 2014      0  \n",
       "..                              ...    ...  \n",
       "159  Mon Nov 24 11:45:44 +0000 2014      1  \n",
       "160  Mon Nov 24 11:45:45 +0000 2014      1  \n",
       "161  Mon Nov 24 11:46:09 +0000 2014      1  \n",
       "162  Mon Nov 24 11:47:49 +0000 2014      1  \n",
       "163  Mon Nov 24 11:49:01 +0000 2014      1  \n",
       "\n",
       "[138 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tw_df = df.iloc[:,[0,1,2,3,8]].copy()\n",
    "src_tw_df = src_tw_df.drop_duplicates()\n",
    "src_tw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-discipline",
   "metadata": {},
   "source": [
    "## Prepocessing\n",
    "- Define function to process text at one time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-consultation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textprocessing(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    text = \" \".join([word for word in text.split() if 'http' not in word\n",
    "                                and not word.startswith('@')])\n",
    "                                #and word != 'RT'])\n",
    "    import re\n",
    "    def remove_punct(tweet):\n",
    "        new_words = []\n",
    "        for word in tweet:\n",
    "            w = re.sub(r'[^\\w\\s]','',word) #remove everything except words and space\n",
    "            w = re.sub(r'_','',w) #how to remove underscore as well\n",
    "            new_words.append(w)\n",
    "\n",
    "        return new_words\n",
    "    text = \"\".join(remove_punct(text))\n",
    "          \n",
    "    from nltk.tokenize import TweetTokenizer\n",
    "    tknzr = TweetTokenizer(strip_handles=True)\n",
    "    text = tknzr.tokenize(text)\n",
    "    \n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    stem = PorterStemmer()\n",
    "    text = [stem.stem(i) for i in text]\n",
    "    \n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words=set(stopwords.words(\"english\"))\n",
    "    def filterstopwords(tw):\n",
    "        filter_stopwords = []\n",
    "        for w in tw:\n",
    "            if w not in stop_words:\n",
    "                filter_stopwords.append(w)\n",
    "        return filter_stopwords\n",
    "    text = filterstopwords(text)\n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-marketplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"original text: \", df.reply_tweet.iloc[107])\n",
    "print(\"cleaned text: \", textprocessing(df.reply_tweet.iloc[107]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-modern",
   "metadata": {},
   "source": [
    "### applied on original data (**df**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-beauty",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_src_tw'] = df.src_tweet.apply(textprocessing)\n",
    "df['cleaned_reply_tw'] = df.reply_tweet.apply(textprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lined-affiliation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-colors",
   "metadata": {},
   "source": [
    "### applied on source tweet data (**src_tw_df**) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-stability",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tw_df['cleaned_src_tw'] = src_tw_df.src_tweet.apply(textprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-husband",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-adoption",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coupled-laugh",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(tweet):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    vs = analyzer.polarity_scores(tweet)\n",
    "    if vs['compound'] >=0.05:\n",
    "        sentiment_label = 'Positive'\n",
    "    elif (vs['compound'] > -0.05) & (vs['compound'] < 0.05):\n",
    "        sentiment_label = 'Neutral'\n",
    "    elif vs['compound']<= -0.05:\n",
    "        sentiment_label = 'Negative' \n",
    "    result = sentiment_label\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-discharge",
   "metadata": {},
   "source": [
    "### applied on original df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-siemens",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['src_sentiment']= df.cleaned_src_tw.apply(get_sentiment)\n",
    "df['reply_sentiment']= df.cleaned_reply_tw.apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convertible-research",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mental-pipeline",
   "metadata": {},
   "source": [
    "### applied on source tweet data only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-volume",
   "metadata": {},
   "outputs": [],
   "source": [
    "### source tweet data only \n",
    "src_tw_df['src_sentiment']=  src_tw_df['cleaned_src_tw'].apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-string",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selected-coordination",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df.label ==1]['reply_sentiment'].value_counts())\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.countplot(x='reply_sentiment',data= df[df['label'] == 1])\n",
    "\n",
    "plt.suptitle('Sentiment analysis on Reply tweets under Catergory Rumour',fontsize=16)\n",
    "plt.title('Reply tweets show more positive attitude towards rumour tweet',fontsize=12,color='grey')\n",
    "plt.savefig('graph/gurlitt/senti_reply_rumour.pdf',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-gallery",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df.label ==0]['reply_sentiment'].value_counts())\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.countplot(x='reply_sentiment',data= df[df['label'] == 0])\n",
    "\n",
    "plt.suptitle('Sentiment analysis on Reply tweets under Catergory Non-Rumour',fontsize=16)\n",
    "plt.title('Reply tweets show more neutrual attitude towards non-rumour tweet',fontsize=12,color='grey')\n",
    "plt.savefig('graph/gurlitt/senti_reply_nonrumour.pdf',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-lewis",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(src_tw_df['src_sentiment'].value_counts())\n",
    "sns.countplot(x='src_sentiment',data =src_tw_df,hue='label')\n",
    "\n",
    "plt.suptitle('Sentiment analysis on Source Tweets',fontsize=16)\n",
    "plt.title('Rumour source tweets show more neutral attitude.',fontsize=12,color='grey')\n",
    "plt.savefig('graph/gurlitt/senti_source.pdf',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-impact",
   "metadata": {},
   "source": [
    "### save data to csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-double",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'D:\\論文\\PHEME9\\Data\\CSV\\gurlitt-df.csv',index=False)\n",
    "src_tw_df.to_csv(r'D:\\論文\\PHEME9\\Data\\CSV\\gurlitt-src.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-childhood",
   "metadata": {},
   "source": [
    "## Keywords Extraction\n",
    "### WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-outside",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-investment",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "mywordcloud = WordCloud().generate(docx)\n",
    "plt.imshow(mywordcloud,interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marked-socket",
   "metadata": {},
   "source": [
    "#### Reply Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-sheep",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-permit",
   "metadata": {},
   "outputs": [],
   "source": [
    "reply_sen = df.cleaned_reply_tw.apply(word_tokenize)\n",
    "reply_tw_list = []\n",
    "for sen in reply_sen:\n",
    "    for token in sen:\n",
    "        reply_tw_list.append(token)\n",
    "reply_tw_doc = \" \".join(reply_tw_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-shepherd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "mywordcloud = WordCloud().generate(reply_tw_doc)\n",
    "plt.imshow(mywordcloud,interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "mywordcloud.to_file('graph/gurlitt/wordcloud_reply.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-midwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive \n",
    "posi_reply_sen = df[df.reply_sentiment=='Positive'].cleaned_reply_tw.apply(word_tokenize)\n",
    "posi_reply_tw_list = []\n",
    "for sen in posi_reply_sen:\n",
    "    for token in sen:\n",
    "        posi_reply_tw_list.append(token)\n",
    "posi_reply_tw_doc = \" \".join(posi_reply_tw_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-purchase",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "mywordcloud = WordCloud().generate(posi_reply_tw_doc)\n",
    "plt.imshow(mywordcloud,interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "mywordcloud.to_file('graph/gurlitt/wordcloud_reply_posi.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-diagnosis",
   "metadata": {},
   "source": [
    "#### Source Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-franchise",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_sen = src_tw_df.cleaned_src_tw.apply(word_tokenize)\n",
    "src_tw_list = []\n",
    "for sen in src_sen:\n",
    "    for token in sen:\n",
    "        src_tw_list.append(token)\n",
    "\n",
    "src_tw_doc = \" \".join(src_tw_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-communications",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "mywordcloud = WordCloud().generate(src_tw_doc)\n",
    "plt.imshow(mywordcloud,interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "mywordcloud.to_file('graph/gurlitt/wordcloud_src.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-installation",
   "metadata": {},
   "source": [
    "### Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-audio",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-photography",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(docx,num=30):\n",
    "    word_tokens = Counter(docx)\n",
    "    most_common = word_tokens.most_common(num)\n",
    "    result = dict(most_common)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-assessment",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(get_tokens(reply_tw_list).items(),columns=['word','freq']).plot(kind='bar',x='word',y='freq')\n",
    "plt.title(\"Most frequent words among reply tweets are 'Prince','show','tonight'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-silicon",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(get_tokens(src_tw_list).items(),columns=['word','freq']).plot(kind='bar',x='word',y='freq')\n",
    "plt.title(\"Most frequent words among source tweets are 'Price', 'show', 'tonight'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-jason",
   "metadata": {},
   "source": [
    "## Emotion Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-moderator",
   "metadata": {},
   "source": [
    "### text2emotion package\n",
    "https://snyk.io/advisor/python/text2emotion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriental-burton",
   "metadata": {},
   "outputs": [],
   "source": [
    "import text2emotion as te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-january",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion = df.cleaned_reply_tw.apply(te.get_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-yield",
   "metadata": {},
   "outputs": [],
   "source": [
    "emo = emotion.apply(lambda x: max(x,key=x.get))\n",
    "df['reply_emotion']=emo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "quarterly-month",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'D:\\論文\\PHEME9\\Data\\CSV\\gurlitt-df.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sapphire-banana",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion1 = src_tw_df.cleaned_src_tw.apply(te.get_emotion)\n",
    "emo1 = emotion.apply(lambda x: max(x,key=x.get))\n",
    "src_tw_df['src_emotion']=emo1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "portable-attempt",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tw_df.to_csv(r'D:\\論文\\PHEME9\\Data\\CSV\\gurlitt-src.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "separate-practice",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'D:\\論文\\PHEME9\\Data\\CSV\\gurlitt-df.csv', encoding = 'utf-8', header = 0)\n",
    "src_tw_df = pd.read_csv(r'D:\\論文\\PHEME9\\Data\\CSV\\gurlitt-src.csv', encoding = 'utf-8', header =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "virtual-birmingham",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tw_df.drop('emotion',axis=1,inplace=True)\n",
    "df.drop('emotion',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-acquisition",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
